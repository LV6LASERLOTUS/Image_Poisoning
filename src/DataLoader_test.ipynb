{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8350a0b-2f01-481c-a0b1-d9f8422032d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2 \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca0cf71-5087-4263-967f-6de74b6c3b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7aa916f-e353-4c78-aa1b-716e9732c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4c2285-4114-496a-8fc9-cafef092b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    A double convolutional Layer with 3x3, ReLU, and optional MaxPooling\n",
    "    \n",
    "    includes a foward method to pass argument forward\n",
    "    \n",
    "    Attributes:\n",
    "    \n",
    "        in_channels (int): The dimensin of the img \n",
    "        out_channels (int): The number of classification\n",
    "        use_pooling (bool): Whether or not to use pooling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels:int, out_channels:int, use_pooling:bool=True):\n",
    "        super().__init__()\n",
    "        self.conv_op=nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        if use_pooling:\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "            \n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "\n",
    "        conv = self.conv_op(x)\n",
    "        pooling = self.pool(conv)\n",
    "        \n",
    "        return pooling\n",
    "\n",
    "class VGG13(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int,num_channels:int):\n",
    "        super().__init__()\n",
    "\n",
    "        # 10 layers of Convolution\n",
    "        self.block1 = DoubleConv(in_channels,64)\n",
    "        self.block2 = DoubleConv(64,128)\n",
    "        self.block3 = DoubleConv(128,256)\n",
    "        self.block4 = DoubleConv(256,512)\n",
    "        self.block5 = DoubleConv(512,512)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(512*7*7,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,num_channels)\n",
    "        ) \n",
    "\n",
    "    def extract_features(self,x)->Tensor:\n",
    "        \n",
    "        down1 = self.block1(x)\n",
    "        down2 = self.block2(down1)\n",
    "        down3 = self.block3(down2)\n",
    "        down4 = self.block4(down3)\n",
    "        down5 = self.block5(down4)\n",
    "        \n",
    "        flatten_features = down5.flatten(start_dim=1)\n",
    "\n",
    "        # Returns a tensor (7*512*512)\n",
    "        return flatten_features\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        flatten_x:torch.tensor = self.extract_features(x)\n",
    "        output = self.fc(flatten_x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15bc13c1-e1d9-4dac-afe2-8dd0577be6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset():\n",
    "    \n",
    "    def __init__(self, root_path:str, transform=None):\n",
    "        self.root_path=root_path\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "        # create a dictionary labeling female_faces,male_faces,object to a number\n",
    "        self.img_labels:dict[str:int]={dir_name:i for i,dir_name in enumerate(sorted(os.listdir(root_path)))}\n",
    "\n",
    "        self.samples:list[tuple(str,int)] = []\n",
    "\n",
    "        # Retrieving each image and saving it as a tuple to samples\n",
    "        for folder_name in os.listdir(self.root_path):\n",
    "            folder_path:str = os.path.join(self.root_path,folder_name)         \n",
    "            for img in os.listdir(folder_path):\n",
    "                try:\n",
    "                    self.samples.append(( os.path.join(folder_path,img), self.img_labels[folder_name]))\n",
    "                except Exception as e:\n",
    "                    print(f'{e}')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        img_path, label = self.samples[index]\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a164be8e-6109-4533-8218-6bc0f274714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(loader):\n",
    "    # VAR[X] = E[X**2] - E[X]**2\n",
    "    channels_sum, channels_squared_sum, num_batches=0,0,0\n",
    "    \n",
    "    for data,_ in loader:\n",
    "        channels_sum = torch.mean(data,dim=[0,2,3])\n",
    "        channels_squared_sum = torch.mean(data**2,dim=[0,2,3])\n",
    "        num_batches+=1\n",
    "\n",
    "    mean = channels_sum/num_batches\n",
    "    std = (channels_squared_sum/num_batches - mean**2)**0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5160d3fc-19ea-4c79-ac8e-2604d809ae7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m ROOT_PATH\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/Images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m FaceDataset(ROOT_PATH)\n\u001b[0;32m----> 6\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m \u001b[43mget_mean_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean             :\u001b[39m\u001b[38;5;124m'\u001b[39m,mean)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStandard Deviatin:\u001b[39m\u001b[38;5;124m'\u001b[39m,std)\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mget_mean_std\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m channels_sum, channels_squared_sum, num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data,_ \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m----> 6\u001b[0m     channels_sum \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     channels_squared_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(data\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m,dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m      8\u001b[0m     num_batches\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Byte"
     ]
    }
   ],
   "source": [
    "# Calculate the Mean & Std for normalizing\n",
    "ROOT_PATH='../data/raw/Images'\n",
    "\n",
    "dataset = FaceDataset(ROOT_PATH)\n",
    "\n",
    "mean, std = get_mean_std(DataLoader(dataset,shuffle=True))\n",
    "\n",
    "print('Mean             :',mean)\n",
    "print('Standard Deviatin:',std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f3589-8024-483e-ad50-472dc40a4a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training Variable\n",
    "\n",
    "ROOT_PATH='../data/raw/Images'\n",
    "IN_CHANNELS=3\n",
    "OUT_CHANNELS=3\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE=0.1\n",
    "EPOCHS=100\n",
    "\n",
    "# Check is nvidia GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224,224),antialias=False),\n",
    "    transforms.Normalize(mean=(9.1111e-05, 1.0778e-04, 1.2251e-04),std=(0.0073, 0.0084, 0.0093))\n",
    "])\n",
    "\n",
    "# Initializing Datasets\n",
    "dataset = FaceDataset(ROOT_PATH,transform)\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_data,attack_data, test_data = random_split(dataset,[0.7,0.1,0.2],generator=generator)\n",
    "print('Total number of dataset:',len(dataset))\n",
    "print('Train                  :',len(train_data))\n",
    "print('Attack                 :', len(attack_data))\n",
    "print('Test                   :', len(test_data))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Setting up \n",
    "model = VGG13(IN_CHANNELS,OUT_CHANNELS).to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21bef77-b4cb-44ae-b238-97f9c9827986",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    correct=0\n",
    "    total=0\n",
    "    \n",
    "    # for idx, (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "    data = data.to(device)\n",
    "    targets = targets.to(device)\n",
    "    # Forward\n",
    "    optimizer.zero_grad()\n",
    "    prediction = model(data)\n",
    "    \n",
    "    loss = criterion(prediction,targets)\n",
    "    \n",
    "    # Backward\n",
    "    \n",
    "    loss.backward()\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     print(name, param.grad.norm())\n",
    "    # print(loss)\n",
    "    # gradient descent\n",
    "    before = model.fc[0].weight.clone().detach()\n",
    "    optimizer.step()\n",
    "    after = model.fc[0].weight.clone().detach()\n",
    "    # print((before - after).abs().sum())\n",
    "    \n",
    "    # Calculate numer of correct classification\n",
    "    correct+=(prediction.argmax(dim=1) == targets).sum().item()\n",
    "    total+=targets.size(0)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{EPOCHS}] | {correct}/{total} | Accuracy: {correct/total:.2f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a685a8-7c9b-441e-af44-a8e4f42d6b89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data, targets = next(iter(train_loader))\n",
    "\n",
    "channels_sum, channels_squared_sum, num_batches=0,0,0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44428a76-17bc-4fe4-a17f-59e4daddf389",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the input image\n",
    "\n",
    "# i=4\n",
    "# img=data[i].cpu()\n",
    "# label = targets[i]\n",
    "\n",
    "# print(label)\n",
    "# plt.imshow(img.per)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24bfbc-4633-4f8b-9c04-c381cdc2bebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing the input image\n",
    "\n",
    "# images, labels = next(iter(train_loader))\n",
    "# print(images.min(), images.max(), images.shape)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b101e3-834d-4585-8539-f698cba7c921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32e311-5877-40cb-90d5-1d40f39cf71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5458e9-84ac-43f5-a9d4-567d997afec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b5870-fc35-4a9e-bcaa-ec3623bd77d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.0-cuda-12.1",
   "language": "python",
   "name": "pytorch-gpu-2.1.0-cuda-12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
